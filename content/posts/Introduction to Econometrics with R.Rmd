---
title: Introduction to Econometrics with R & Morden Drive
author: Ziwen
date: "2021-03-09" ## Or "Lecture no."
urlcolor: blue
mainfont: cochineal
sansfont: Fira Sans
monofont: Fira Code ## Although, see: https://tex.stackexchange.com/q/294362
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, dpi=300)
rm(list=ls())
```

source: [link](https://scpoecon.github.io/ScPoEconometrics/)


# Data 
**data type**: numeric integer, complex(复数), logical, character, categorical (factor)

## Data structure

| Dimension | Homogeneous | Heterogeneous |
| :-----:| :----: | :----: |
| 1 | vector | list |
| 2 | matrix | data frame |
| 3+ | array|inserted lists|

A vector is a one-dimensional array. A matrix is a two-dimensional array. In R you can create arrays of arbitrary dimensionality N. Here is how:

```{r array}
library()
d = 1:16
d3 = array(data = d,dim = c(4,2,2)) # d3 是四行两列，然后有两页
d4 = array(data = d,dim = c(4,2,2,3))  # will recycle 1:16 
d3
#Similary, d4 would have two pages, and another 3 registers in a fourth dimension
```


A list is a one-dimensional heterogeneous data structure. So it is indexed like a vector with a single integer value (or with a name), but each element can contain an element of any type. Lists are similar to a python or julia Dict object. Many R structures and outputs are lists themselves. Lists are extremely useful and versatile objects, so make sure you understand their useage. The `[]` syntax returns a list, while the `[[]]` returns an element of a list.

```{r list}

ex_list = list(
  a = c(1, 2, 3, 4),
  b = TRUE,
  c = "Hello!",
  d = function(arg = 42) {print("Hello World!")},
  e = diag(5)
)
ex_list[1] #returns a list contain the first element.
ex_list[[1]] #returns the first element of the list, in this case, a vector
```


## Data frame

Unlike a matrix, which can be thought of as a vector rearranged into rows and columns, a data frame is not required to have the same data type for each element. A data frame is a list of vectors, and each vector has a name. So, each vector must contain the same data type, but the different vectors can store different data types. Note, however, that all vectors must have the **same length** (differently from a list)! 

A `data.frame` is similar to a typical Spreadsheet. There are rows, and there are columns. A row is typically thought of as an observation, and each column is a certain variable, characteristic or feature of that observation.

```{r dataframe}

example_data = data.frame(x = c(1, 3, 5, 7, 9, 1, 3, 5, 7, 9),
                          y = c(rep("Hello", 9), "Goodbye"),
                          z = rep(c(TRUE, FALSE), 5))

# 安装包所在的位置的数据
 #read csv 是常用的读取data frame 用的
#To look at data in a data.frame, we have two useful commands: head() and str().
#head() 帮助看数据的变量和结构

head(mtcars)
```

The function `str()` will display the "structure" of the data frame. It will display the number of observations and variables, list the variables, give the type of each variable, and show some elements of each variable. This information can also be found in the "Environment' window in RStudio.
```{r preview}
str(mtcars)
#To quickly obtain a vector of the variable names, we use the names() function.
names(mtcars)
#Subsetting data frames can work much like subsetting matrices using square brackets, [ , ]
mtcars[mtcars$mpg > 20, c("cyl", "disp", "wt")] #多个条件并列
```
      
      
# Work with data

## Summary statistics
```{r,include=FALSE}
library(ggplot2)
mpg$cty
```

| Measure            | `R`              | Result     |
| ------------------ | ---------------- | ---------- |
| Variance           | `var(mpg$cty)`   | 18.1130736 |
| Standard Deviation | `sd(mpg$cty)`    | 4.2559457  |
| IQR                | `IQR(mpg$cty)`   | 5          |
| Minimum            | `min(mpg$cty)`   | 9          |
| Maximum            | `max(mpg$cty)`   | 35         |
| Range              | `range(mpg$cty)` | 9, 35      |
| Mean    | `mean(mpg$cty)`   | 16.8589744 |
| Median  | `median(mpg$cty)` | 17         |

```{r table}
table(mpg$drv)
# output the observation numbers of each species
table(mpg$drv)/nrow(mpg)
#output the percent of each category; nrow is the sum of obs
```

## Plotting
We will look at four methods of visualizing data by using the basic plot facilities built-in with R:

* Histograms
* Barplots
* Boxplots
* Scatterplots

```{r plotting}
layout(matrix(c(1,2,3,4),nr=2,byrow=T))
hist(mpg$cty,
     xlab   = "Miles Per Gallon (City)",
     main   = "Histogram of MPG (City)", # main title
     breaks = 12,   # how many breaks?
     col    = "blue",
     border = "black")

barplot(table(mpg$drv),
        xlab   = "Drivetrain (f = FWD, r = RWD, 4 = 4WD)",
        ylab   = "Frequency",
        main   = "Drivetrains",
        col    = "blue",
        border = "black")
#However, more often we will use boxplots to compare a numerical variable 
#for different values of a categorical variable.
boxplot(hwy ~ drv, data = mpg,
     xlab   = "Drivetrain (f = FWD, r = RWD, 4 = 4WD)",
     ylab   = "Miles Per Gallon (Highway)",
     main   = "MPG (Highway) vs Drivetrain",
     pch    = 20,
     cex    = .5,
     col    = "blue",
     border = "black") #y~x

plot(hwy ~ displ, data = mpg,
     xlab = "Engine Displacement (in Liters)",
     ylab = "Miles Per Gallon (Highway)",
     main = "MPG (Highway) vs Engine Displacement",
     pch  = 20, # 点的形状
     cex  = .5, # 越小点和文字越小
     col  = "blue")
# plot draws the scatter plots with y against x
```

see [link](https://www.statmethods.net/advgraphs/parameters.html) for detals of plotting parameters.

```{r ggplot,warning=FALSE,message=FALSE}
ggplot(data = mpg, aes(x=displ,y=hwy)) +   # ggplot() makes base plot
  geom_point(color="blue",size=2) +     # how to show x and y? #geom_boxplot
  ylab("Miles Per Gallon (Highway)") +  # name of y axis
  xlab("Engine Displacement (in Liters)") + # x axis
  theme_bw() +    # change the background
  geom_smooth(method = "loess",se=TRUE)+
  ggtitle("MPG (Highway) vs Engine Displacement")   # add a title

```

## tidyverse

[cheat sheet](https://www.dropbox.com/s/2tvzyy1sy1920y5/tidyverse%20cheat%20sheet.pdf?dl=0)


```{r tidyverse,message=FALSE}
library(readr)  # you need `install.packages("readr")` once!
path = system.file(package="ScPoEconometrics","datasets","example-data.csv")


#subset
mpg[mpg$hwy>35,c("manufacturer","model","year")]
# another way
subset(mpg, subset = hwy > 35, select = c("manufacturer", "model", "year"))
# another way
```

Deal with non-tidy data tibble is non-tidy data

```{r non-tidy}
library(readxl)  # load the library
# Notice that if you installed the R package of this book,
# you have the .xls data file already at 
# `system.file(package="ScPoEconometrics",
#                        "datasets","demo_gind.xls")`
# otherwise:
# * download the file to your computer
# * change the argument `path` to where you downloaded it
# you may want to change your working directory with `setwd("your/directory")
# or in RStudio by clicking Session > Set Working Directory
library(ScPoApps)
packageVersion("ScPoApps")
# total population in raw format
```

It's important to know how the data is organized in the spreadsheet. Open the file with Excel to see:

- There is a heading which we don't need.
- There are 5 rows with info that we don't need.
- There is one table per variable (total population, males, females, etc)
- Each table has one row for each country, and one column for each year.
- As such, this data is **not tidy**.



With `dplyr` you can do the following operations on `data.frame`s and `tibble`s:

- Choose observations based on a certain value (i.e. subset): `filter()`
- Reorder rows: `arrange()`
- Select variables by name: `select()`
- Create new variables out of existing ones: `mutate()`
- Summarise variables: `summarise()`

All of those verbs can be used with `group_by()`, where we apply the respective operation on a *group* of the dataframe/tibble. For example, on our `tot_pop` tibble we will now

- filter
- mutate
- and plot the resulting values

Let’s get a plot of the populations of France, the UK and Italy over time, in terms of millions of people. We will make use of the `piping` syntax of `dplyr` which we introduced just above.






>其中paste函数式paste函数的缩减版本，少了一个参数sep。参数中的...表示的是想要拼接的对象，后面的参数表示拼接的方式。通常对于字符串拼接我们有三种需求：
一堆单独的字符串拼接到一起；
两个或者更多字符串对象根据元素对应关系拼接到一起；
一个字符串连接在一起。当处理前两种需求的时候用到的参数是sep，即多个字符串之间的拼接；当处理第三种需求的时候用到的参数是collapse，即一个字符串拼接的时候用什么符号或者格式来连接；

```{r paste}
paste('hello','world','!')  #一堆单独的字符串拼接到一起
paste(c('A','B','C'),c(1,2,3)) #两个或者更多字符串对象根据元素对应关系拼接到一起
paste(1:10) #一个字符串连接在一起


paste('hello','world','!')  #一堆单独的字符串拼接到一起，如果想用‘_’连接起来
paste('hello','world','!',sep = '_')
## [1] "hello_world_!"
paste(c('A','B','C'),c(1,2,3)) #两个或者更多字符串对象根据元素对应关系拼接到一起，如果想用‘-’连接起来
paste(c('A','B','C'),c(1,2,3),sep = '-')
## [1] "A-1" "B-2" "C-3"
#此外需要注意的是如果传入的两个或多个对象的长度不一样，对应关系会发生变化，比如
paste(c('A','B','C','D','E'),c(1,2),sep = '-')
## [1] "A-1" "B-2" "C-1" "D-2" "E-1"
paste(1:10) 
#一个字符串连接在一起，这第三种需求与上面很大的不同在于上面两种需求传入的都是多个对象，
#而第三种需求在函数的第一个参数位置上只传入一个对象，如果需要用‘~’连接
paste(1:10, collapse = '~')
```





# Regression
Analysis of Variance (ANOVA) refers to a method to decompose variation in one variable as a function of several others. We can use this idea on our outcome $y$. Suppose we wanted to know the variance of $y$, keeping in mind that, by definition, $y_i =\hat y_i+e_i$ We would write $Var(y)=Var(\hat y+e)=Var(\hat y)+Var(e)+Cov(\hat y,e)=Var(\hat y)+Var(e)$
```{r wage}
data("wage1", package = "wooldridge")   # load data

# a function that returns a plot
plotfun <- function(wage1,log=FALSE,rug = TRUE){#指定两个变量log rug作为test expression
    y = wage1$wage
    if (log){#false
        y = log(wage1$wage)
    }
    plot(y = y,
       x = wage1$educ, 
       col = "red", pch = 21, bg = "grey",     
       cex=1.25, xaxt="n", frame = FALSE,      # set default x-axis to none
       main = ifelse(log,"log(Wages) vs. Education, 1976","Wages vs. Education, 1976"),
       xlab = "years of education", 
       ylab = ifelse(log,"Log Hourly wages","Hourly wages"))
    axis(side = 1, at = c(0,6,12,18))         # add custom ticks to x axis
    if (rug) rug(wage1$wage, side=2, col="red")        # add `rug` to y axis
}

par(mfcol = c(2,1))  # set up a plot with 2 panels
# plot 1: standard scatter plot
plotfun(wage1)

# plot 2: add a panel with histogram+density
hist(wage1$wage,prob = TRUE, col = "grey", border = "red", 
     main = "Histogram of wages and Density",xlab = "hourly wage")
lines(density(wage1$wage), col = "black", lw = 2)



hourly_wage <- lm(formula = wage ~ educ, data = wage1)
#update lm的名称
log_hourly_wage = update(hourly_wage, log(wage) ~ ., data = wage1)
par(mfrow=c(1,2))
plotfun(wage1,rug=FALSE)
abline(hourly_wage,col="black",lw=2)
plotfun(wage1,log=TRUE,rug = FALSE) #因为前面定义了初始状态下log rug两个参数的T/F，可以在这从新给定
abline(log_hourly_wage, col="black",lw=2)
```

```{r}
data("ceosal1", package = "wooldridge")  
par(mfrow = c(1,2))
plot(salary ~ sales, data = ceosal1, main = "Sales vs Salaries",xaxt = "n",frame = FALSE)
#xaxt means there is no x-axis/frame 画框
axis(1, at = c(0,40000, 80000)) # add axis in x-side 3:upper 4:right
rug(ceosal1$salary,side = 2)
rug(ceosal1$sales,side = 1)
plot(log(salary) ~ log(sales), data = ceosal1, main = "Log(Sales) vs Log(Salaries)")
rug(log(ceosal1$salary),side = 2)
rug(log(ceosal1$sales),side = 1)
```

# multivariate regression

```{r,results="asis"}
educ_only <- lm(lwage ~ educ                 , data = wage1)
educ_exper <- lm(lwage ~ educ + exper        , data = wage1)
log_wages <- lm(lwage ~ educ + exper + tenure, data = wage1)
stargazer::stargazer(educ_only, educ_exper, log_wages,type="html")
```

# catogorical variables

```{r dummy}
data("wage1",package = "wooldridge")
library(patchwork)
p1<-wage1 %>% ggplot(aes(x=educ,y=lwage,color=as.factor(female),group=(as.factor(female))))+
  geom_point(size=1 ,shape=1)+geom_smooth(method="lm",se=T)
p2<-wage1 %>% ggplot(mapping=aes(x=educ,y=lwage))+geom_point(size=1)+geom_smooth(method="lm",se=T)

p1|p2

```


# Modern Drive

[statistical inference via data science](https://moderndive.com/index.html)



```{r,results="asis",message=FALSE}
library("dplyr")
library("nycflights13")
library("AER") # Applied Econometrics with R
library("stargazer")

daily <- flights %>%
  filter(origin == "EWR") %>%
  group_by(year, month, day) %>%
  summarise(delay = mean(dep_delay, na.rm = TRUE))

daily_weather <- weather %>%
  filter(origin == "EWR") %>%
  group_by(year, month, day) %>%
  summarise(temp   = mean(temp, na.rm = TRUE),
            wind   = mean(wind_speed, na.rm = TRUE),
            precip = sum(precip, na.rm = TRUE))

# Merge flights with weather data frames
both <- daily %>%
  inner_join(y = daily_weather, by = c("year", "month", "day")) %>% 
  data.frame()  # Temporary fix

# Create an indicator for quarter 学习一下
both$quarter <- cut(both$month, breaks = c(0, 3, 6, 9, 12), 
                                labels = c("1", "2", "3", "4"))

# Create a vector of class logical
both$hot <- as.logical(both$temp > 85)

head(both)
stargazer(both, type = "html") #对于dataframe 
```




`inner_join()` function to join the two data frames, where the rows will be matched by the variable carrier, and then compare the resulting data frames:




| Verb           | Data wrangling operation                                     |
| :------------- | :----------------------------------------------------------- |
| `filter()`     | Pick out a subset of rows                                    |
| `summarize()`  | Summarize many values to one using a summary statistic function like `mean()`, `median()`, etc. |
| `group_by()`   | Add grouping structure to rows in data frame. Note this does not change values in data frame, rather only the meta-data |
| `mutate()`     | Create new variables by mutating existing ones               |
| `arrange()`    | Arrange rows of a data variable in ascending (default) or `desc`ending order |
| `inner_join()` | Join/merge two data frames, matching rows by a key variable  |


"Tidy" data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:

* Each variable forms a column.
* Each observation forms a row.
* Each type of observational unit forms a table.


In this book so far, you’ve only seen data frames that were already in “tidy” format. Furthermore, for the rest of this book, you’ll mostly only see data frames that are already in “tidy” format as well. This is not always the case however with all datasets in the world. If your original data frame is in wide (non-“tidy”) format and you would like to use the `ggplot2` or `dplyr` packages, you will first have to convert it to “tidy” format. To do so, we recommend using the `pivot_longer()` function in the `tidyr` package








# Some Practices and Regressions 

[Principles of Economitrics with R](https://bookdown.org/ccolonescu/RPoE4/intro.html)

## Interval estimation and hypotheses testing
 An interval estimate of b2b2 based on the `t-ratio` is calculated in Equation 4, which we can consider as "an interval that includes the true parameter with a probability of $100(1-\alpha)%$" In this context, $\alpha$ is called `significance level`, and the interval is called, for example, `a 95% confidence interval estimate for` $\beta_2$. The critical value of the `t-ratio`, $t_c$, depends on the chosen significance level and on the number of degrees of freedom. In $R$, the function that returns critical values for the $t$ distribution is $qt(1−\frac{\alpha}{2},df)$, where $df$ is the number of degrees of freedom.

$$b_2+/-t_c\times se(b_2) (4)$$

**A side note about using distributions in RR.** There are four types of functions related to distributions, each type’s name beginning with one of the following four letters: `p` for the cummulative distribution function, `d` for density, `r` for a draw of a random number from the respective distribution, and `q` for quantile. This first letter is followed by a few letters suggesting what distribution we refer to, such as `norm`, `t`, `f`, and `chisq`. Now, if we put toghether the first letter and the distribution name, we get functions such as the following, where `x` and `q` stand for quantiles, `p` stands for probability, `df` is degree of freedom (of which FF has two), `n` is the desired number of draws, and `lower.tail` can be TRUE (default) if probabilities are $P[X\leq x]$ or FALSE if probabilities are $P[X>x]$:

- For the uniform distribution:
  - dunif(x, min = 0, max = 1)
  - punif(q, min = 0, max = 1, lower.tail = TRUE)
  - qunif(p, min = 0, max = 1, lower.tail = TRUE)
  - runif(n, min = 0, max = 1)
- For the normal distribution:
  - dnorm(x, mean = 0, sd = 1)
  - pnorm(q, mean = 0, sd = 1, lower.tail = TRUE)
  - qnorm(p, mean = 0, sd = 1, lower.tail = TRUE)
  - rnorm(n, mean = 0, sd = 1)
- For the $t$ distribution:
  - dt(x, df)
  - pt(q, df, lower.tail = TRUE)
  - qt(p, df, lower.tail = TRUE)
  - rt(n, df)
- For the $F$ distribution:
  - df(x, df1, df2)
  - pf(q, df1, df2, lower.tail = TRUE)
  - qf(p, df1, df2, lower.tail = TRUE)
  - rf(n, df1, df2)
- For the $\chi^2$ distribution:
  - dchisq(x, df)
  - pchisq(q, df, lower.tail = TRUE)
  - qchisq(p, df, lower.tail = TRUE)
  - rchisq(n, df)
  
  $$\hat \sigma^2=\frac{\Sigma \hat e_i^2}{N-2}$$
  $$t=\frac{b_2-\beta_2}{se(b_2)}$$
  
```{r confidence}
library(xtable)
library(PoEdata)
library(knitr)

data("food")
alpha <- 0.05 # chosen significance level
mod1 <- lm(food_exp~income, data=food)
b2 <- coef(mod1)[[2]]
df <- df.residual(mod1) # degrees of freedom
smod1 <- summary(mod1)
seb2 <- coef(smod1)[2,2] # se(b2)
tc <- qt(1-alpha/2, df) # quantile
lowb <- b2-tc*seb2  # lower bound
upb <- b2+tc*seb2   # upper bound


ci<-confint(mod1)
print(ci)

lowb_b2 <- ci[2, 1] # lower bound
upb_b2 <- ci[2, 2]  # upper bound.
```

```{r}
data("andy",package="PoEdata")
mod1 <- lm(sales~price+advert, data=andy)
smod1 <- data.frame(xtable(summary(mod1)))
kable(smod1, 
caption="The basic multiple regression model", 
col.names=c("coefficient", "Std. Error", "t-value", "p-value"),
align="c", digits=3)
library(effects)
effprice <- effect("price", mod1)
plot(effprice)
alleffandy <- allEffects(mod1)
plot(alleffandy)
anova(mod1) #analysis of variance
```


## IV
A strong instrument, one that is highly correlated with the endogenous regressor it concerns, reduces the variance of the estimated coefficient.



The results for the wage equation are as follows:

1. Weak instruments test: rejects the null, meaning that at least one instrument is strong
2. (Wu-)Hausman test for endogeneity: barely rejects the null that the variable of concern is uncorrelated with the error term, indicating that  educ  is marginally endogenous
3. Sargan overidentifying restrictions: does not reject the null, meaning that the extra instruments are valid (are uncorrelated with the error term).

```{r IV}
rm(list=ls()) #Removes all items in Environment!
library(AER) #for `ivreg()`
library(lmtest) #for `coeftest()` and `bptest()`.
library(broom) #for `glance(`) and `tidy()`
library(PoEdata) #for PoE4 datasets
library(car) #for `hccm()` robust standard errors
library(sandwich)
library(knitr) #for making neat tables with `kable()`
library(stargazer) 
data("mroz", package="PoEdata")
mroz1 <- mroz[mroz$lfp==1,] #restricts sample to lfp=1
educ.ols <- lm(educ~exper+I(exper^2)+mothereduc, data=mroz1)
kable(tidy(educ.ols), digits=4, align='c',caption=
  "First stage in the 2SLS model for the 'wage' equation")
```
The  p -value for  `mothereduc`  is very low (see Table 10.1), indicating a strong correlation between this instrument and the endogenous variable  `educ`  even after controling for other variables. The second stage in the two-stage procedure is to create the fitted values of `educ`  from the first stage and plug them into the model of interest, to replace the original variable  `educ`.
```{r}
educHat <- fitted(educ.ols)
wage.2sls <- lm(log(wage)~educHat+exper+I(exper^2), data=mroz1)
kable(tidy(wage.2sls), digits=4, align='c',caption=
  "Second stage in the 2SLS model for the 'wage' equation")
```
The results of the explicit  2SLS  procedure are shown in Table 10.2; keep n mind, however, that the standard errors calculated in this way are incorrect; the correct method is to use a dedicated software function to solve an instrumental variable model. In  R , such a function is ivreg()

```{r,results="asis"}
data("mroz", package="PoEdata")
mroz1 <- mroz[mroz$lfp==1,] #restricts sample to lfp=1.
mroz1.ols <- lm(log(wage)~educ+exper+I(exper^2), data=mroz1)
mroz1.iv <- ivreg(log(wage)~educ+exper+I(exper^2)|
            exper+I(exper^2)+mothereduc, data=mroz1)
mroz1.iv1 <- ivreg(log(wage)~educ+exper+I(exper^2)|
            exper+I(exper^2)+mothereduc+fathereduc,
            data=mroz1)
stargazer(mroz1.ols, wage.2sls, mroz1.iv, mroz1.iv1,
  title="Wage equation: OLS, 2SLS, and IV models compared",
  header=FALSE, 
  type="html", # "html" or "latex" (in index.Rmd) 
  keep.stat="n",  # what statistics to print
  omit.table.layout="n",
  star.cutoffs=NA,
  digits=4, 
#  single.row=TRUE,
  intercept.bottom=FALSE, #moves the intercept coef to top
  column.labels=c("OLS","2SLS", "IV", 
                  "IV +fathereduc")) #supresses the stars)

```

# Panel data model

```{r,include=FALSE}
rm(list=ls()) #Removes all items in Environment!
library(plm) 
library(tseries) # for `adf.test()`
library(dynlm) #for function `dynlm()`
library(vars) # for function `VAR()`
library(nlWaldTest) # for the `nlWaldtest()` function
library(lmtest) #for `coeftest()` and `bptest()`.
library(broom) #for `glance(`) and `tidy()`
library(PoEdata) #for PoE4 datasets
library(car) #for `hccm()` robust standard errors
library(sandwich)
library(knitr) #for `kable()`
library(forecast) 
library(systemfit)
library(AER)
library(xtable)
```


step 1: set the data

```{r}
library(xtable)
data("nls_panel", package="PoEdata")
nlspd <- pdata.frame(nls_panel, index=c("id", "year")) #set time and id
smpl <- nlspd[nlspd$id %in% c(1,2),c(1:6, 14:15)]
tbl <- xtable(smpl) 
kable(tbl, digits=4, align="c",
      caption="A data sample")
pdim(nlspd) # examine the balance
```


A **pooled** model has the specification in Equation  1 , which does not allow for intercept or slope differences among individuals. Such a model can be estimated in *R*  using the specification pooling in the `plm()` function, as the following code sequence illustrates.

关于coefest() 里面的argument When `type = "const"` constant variances are assumed and and `covHC` gives the usual estimate of the covariance matrix of the coefficient estimates:
All other methods do not assume constant variances and are suitable in case of heteroskedasticity. `"HC"` gives White's estimator; for details see the references. `vcov=vcovHC` Heteroskedasticity-consistent estimation of the covariance matrix of the coefficient estimates in a linear regression model.

```{r,results="asis"}
wage.pooled <- plm(lwage~educ+exper+I(exper^2)+
  tenure+I(tenure^2)+black+south+union, 
  model="pooling", data=nlspd)
kable(tidy(wage.pooled), digits=3, 
           caption="Pooled model")
#vcov covariance matirx
# type="HC0"  White standard error
tbl <- tidy(coeftest(wage.pooled, vcov=vcovHC(wage.pooled,
                    type="HC0",cluster="group")))
kable(tbl, digits=5, caption=
"Pooled 'wage' model with cluster robust standard errors")

```


The fixed effects model takes into account individual differences, translated into different intercepts of the regression line for different individuals. The model in this case assigns the subscript `i`  to the constant term $\beta_1$; the constant terms calculated in this way are called **fixed effects**.

```{r fixed-effecr}

nls10 <- pdata.frame(nls_panel[nls_panel$id %in% 1:10,]) # set panel data


wage.fixed <- lm(lwage~exper+I(exper^2)+
                  tenure+I(tenure^2)+union+factor(id),
                  data=nls10)
kable(tidy(wage.fixed), digits=3, 
      caption="Fixed effects in a subsample")
```


The function `factor()` generates dummy variables for all categories of the variable, taking the first category as the reference. To include the reference in the output, one needs to exclude the constant from the regression model by including the term  −1  in the regression formula. When the constant is not excluded, the coefficients of the dummy variables represent, as usual, the difference between the respective category and the benchmark one.

However, to estimate a fixed effects in  `R`  we do not need to create the dummy variables, but use the option `model="within"` in the `plm()` function. The following code fragment uses the whole sample. 不减1 得到的是就和fixed effect 一样，就是和respective category 和benchmark 的差异

```{r}
wage.within <- plm(lwage~exper+I(exper^2)+
                  tenure+I(tenure^2)+south+union,
                  data=nlspd, 
                  model="within")
tbl <- tidy(wage.within)
kable(tbl, digits=3, caption=
"Fixed effects using 'within' with full sample")
```


[another resource](https://www.econometrics-with-r.org/index.html)

#


Testing if fixed effects are necessary is to compare the fixed effects model wage.within with the pooled model wage.pooled. The function `pFtest()` does this comparison, as in the following code lines.

```{r compare fixed vs pooled}
kable(tidy(pFtest(wage.within, wage.pooled)), caption=
        "Fixed effects test: Ho:'No fixed effects'")
```

null hypothesis=no fixed effect


## random effect 

The same function we used for fixed effects can be used for random effects, but setting the argument model= to 'random' and selecting the `random.method` as one out of four possibilities: "swar" (default), "amemiya", "walhus", or "nerlove". The random effects test function is `plmtest()`, which takes as its main argument the pooling model (indeed it extracts the residuals from the pooling object).

```{r}
wageReTest <- plmtest(wage.pooled, effect="individual")
kable(tidy(wageReTest), caption=
        "A random effects test for the wage equation")

```

null hypothesis of zero variance in individual-specific errors is rejected; therefore, heterogeneity among individuals may be significant. 虚无假设是没有random effect


Random effects estimators are reliable under the assumption that individual characteristics (heterogeneity) are exogenous, that is, they are independent with respect to the regressors in the random effects equation. The same Hausman test for endogeneity we have already used in another chapter can be used here as well, with the null hypothesis that individual random effects are exogenous. The test function `phtest()` compares the fixed effects and the random effects models; the next code lines estimate the random effects model and performs the Hausman endogeneity test.  随机效应需要个人变量是外生的（每个个人和regressor之间没有关系）

```{r}
wage.random <- plm(lwage~educ+exper+I(exper^2)+
                  tenure+I(tenure^2)+black+south+union,
                  data=nlspd, random.method="swar",
                  model="random")
kable(tidy(wage.random), digits=4, caption=
      "The random effects results for the wage equation")
kable(tidy(phtest(wage.within, wage.random)), caption=
 "Hausman endogeneity test for the random effects wage model")
```


shows a low p-value of the test, which indicates that the null hypothesis saying that the individual random effects are exogenous is rejected, which makes the random effects equation inconsistent. In this case the fixed effects model is the correct solution. (The number of parameters in Table 15.10 is given for the time-varying variables only.) 可以拒绝虚无假设：随机效应是外生的

The fixed effects model, however, **does not allow time-invariant variables such as  educ  or  black.** Since the problem of the random effects model is endogeneity, one can use instrumental variables methods when time-invariant regressors must be in the model. The Hausman-Taylor estimator uses instrumental variables in a random effects model; it assumes four categories of regressors: time-varying exogenous, time-varying endogenous, time-invariant exogenous, and time-invariant endogenous. The number of time-varying variables must be at least equal to the number of time-invariant ones. In our  wage  model, suppose  exper ,  tenure  and  union  are time-varying exogenous,  south  is time-varying endogenous,  black  is time-invariant exogenous, and  educ  is time-invariant endogenous. The same `plm()` function allows carrying out Hausman-Taylor estimation by setting model= “ht”.


A test to see if the coefficients are significantly different between the pooling and fixed effects equations can be done in  R  using the function `pooltest` from package `plm`; to perform this test, the fixed effects model should be estimated with the function `pvcm` with the argument `model= "within"`, as the next code lines show.

感觉还是很不清楚怎么用r分析fixed effect model: 小小的summary一下



